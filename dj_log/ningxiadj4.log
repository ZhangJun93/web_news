2017-03-16 06:00:09 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: web_news)
2017-03-16 06:00:09 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'web_news.spiders', 'SPIDER_MODULES': ['web_news.spiders'], 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS_PER_DOMAIN': 32, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'CONCURRENT_REQUESTS': 32, 'METAREFRESH_ENABLED': False, 'CONCURRENT_REQUESTS_PER_IP': 32, 'BOT_NAME': 'web_news', 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'REDIRECT_ENABLED': False, 'LOG_FILE': 'dj_log/ningxiadj4.log', 'DOWNLOAD_DELAY': 0.25}
2017-03-16 06:00:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.closespider.CloseSpider',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2017-03-16 06:00:09 [ningxiadj] INFO: get key 6
2017-03-16 06:00:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-16 06:00:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-16 06:00:09 [scrapy.middleware] INFO: Enabled item pipelines:
['web_news.pipelines.MongoDBPipeline']
2017-03-16 06:00:09 [scrapy.core.engine] INFO: Spider opened
2017-03-16 06:00:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-16 06:00:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <400 http://www.nxdjw.gov.cn/../gbgz/index.html>: HTTP status code is not handled or not allowed
2017-03-16 06:00:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <400 http://www.nxdjw.gov.cn/../djyj/index.html>: HTTP status code is not handled or not allowed
2017-03-16 06:00:12 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-16 06:00:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3712,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 43549,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 7,
 'downloader/response_status_count/400': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 15, 22, 0, 12, 332163),
 'item_scraped_count': 5,
 'log_count/INFO': 10,
 'request_depth_max': 4,
 'response_received_count': 9,
 'scheduler/dequeued/redis': 9,
 'scheduler/enqueued/redis': 18,
 'start_time': datetime.datetime(2017, 3, 15, 22, 0, 9, 260200)}
2017-03-16 06:00:12 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-17 06:00:04 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: web_news)
2017-03-17 06:00:04 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'web_news.spiders', 'SPIDER_MODULES': ['web_news.spiders'], 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS_PER_DOMAIN': 32, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'CONCURRENT_REQUESTS': 32, 'METAREFRESH_ENABLED': False, 'CONCURRENT_REQUESTS_PER_IP': 32, 'BOT_NAME': 'web_news', 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'REDIRECT_ENABLED': False, 'LOG_FILE': 'dj_log/ningxiadj4.log', 'DOWNLOAD_DELAY': 0.25}
2017-03-17 06:00:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.closespider.CloseSpider',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2017-03-17 06:00:04 [twisted] ERROR: Unhandled error in Deferred:
2017-03-17 06:00:04 [twisted] ERROR: Unhandled Error
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1237, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1099, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u231/spider/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u231/spider/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 573, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 585, in parse_response
    response = connection.read_response()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 582, in read_response
    raise response
redis.exceptions.ResponseError: OOM command not allowed when used memory > 'maxmemory'.

2017-03-18 06:00:39 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: web_news)
2017-03-18 06:00:39 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'web_news.spiders', 'SPIDER_MODULES': ['web_news.spiders'], 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS_PER_DOMAIN': 32, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'CONCURRENT_REQUESTS': 32, 'METAREFRESH_ENABLED': False, 'CONCURRENT_REQUESTS_PER_IP': 32, 'BOT_NAME': 'web_news', 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'REDIRECT_ENABLED': False, 'LOG_FILE': 'dj_log/ningxiadj4.log', 'DOWNLOAD_DELAY': 0.25}
2017-03-18 06:00:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.closespider.CloseSpider',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2017-03-18 06:00:40 [ningxiadj] INFO: get key 4
2017-03-18 06:00:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-18 06:00:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-18 06:00:40 [scrapy.middleware] INFO: Enabled item pipelines:
['web_news.pipelines.MongoDBPipeline']
2017-03-18 06:00:40 [scrapy.core.engine] INFO: Spider opened
2017-03-18 06:00:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-18 06:00:40 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-18 06:00:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 568,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 31056,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 17, 22, 0, 40, 901102),
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued/redis': 2,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2017, 3, 17, 22, 0, 40, 61180)}
2017-03-18 06:00:40 [scrapy.core.engine] INFO: Spider closed (finished)
